# Smriti.ai Project Report

## 1. Project Overview
**Smriti.ai** is an advanced memory augmentation system designed to act as a "second brain." It continuously records audio context, identifies speakers, and stores conversations in a searchable vector database. Users can query this memory using voice commands, and the system responds with answers generated by an LLM (Gemini), synthesized into speech. The system is a multi-component ecosystem comprising a central server, a wearable hardware client, and a mobile companion app for management.

## 2. Architecture Overview
The system follows a distributed Client-Server architecture with a mobile management layer:

-   **Hardware Client (Wearable)**: A Raspberry Pi-based device equipped with a camera, microphone (via Bluetooth earbuds), and push-buttons. It captures the user's environment (audio/video) and handles user interaction.
-   **Server (Python/Flask)**: The central processing hub that handles heavy lifting: AI inference, database management, and API services.
-   **Mobile App (Flutter)**: A companion application for managing the system, specifically for labeling unknown speakers and viewing enrolled profiles.

## 3. Key Features
-   **Real-time Context Recording**: Continuously listens, transcribes, and stores conversations from the wearable device.
-   **Speaker Diarization & Identification**: Distinguishes between different speakers and identifies them.
-   **Voice Query & Response**: Users can ask questions about past conversations via the earbuds ("What did Raj say about the project yesterday?").
-   **Face Recognition**: Identifies people in the camera's view and announces their names to the user.
-   **Automated Alerts & Reminders**: Analyzes conversations to detect and extract reminders, alarms, or notifications (e.g., "Remind me to call John at 5 PM").
-   **Auto-Enrollment**: Automatically creates profiles for unknown faces and voices.
-   **Mobile Management Dashboard**:
    -   View list of all speakers and faces.
    -   Listen to audio samples of unknown speakers.
    -   Label "Unknown" speakers/faces with real names.
    -   View images of enrolled people.

---

## 4. Comprehensive Implementation Details

### 4.1. Server & API (`server.py`)
The core is a Flask application wrapped with `flask_socketio` for real-time bidirectional communication.
-   **Streaming Audio**:
    -   **Query Stream**: `audio_chunk` events receive PCM audio from the earbuds. On `audio_complete`, it triggers the STT -> LLM -> TTS pipeline.
    -   **Context Stream**: `context_audio_chunk` events receive background audio. It accumulates audio into 50-second segments (with 2s overlap) and processes them in the background.
-   **Face Endpoint**: `/process_face` (POST) accepts an image, detects faces, generates embeddings, and matches them against Pinecone.

### 4.2. Hardware Client (Raspberry Pi)
The physical interface for the user.
-   **Hardware Components**:
    -   **Raspberry Pi**: Central compute unit for the client.
    -   **Camera Module**: Captures images for face recognition.
    -   **Bluetooth Earbuds**: Acts as the microphone for input (context/queries) and speaker for output (TTS responses).
    -   **Push-Buttons**: Physical controls to trigger specific actions (e.g., "Start Query", "Capture Image").
-   **Operation**:
    -   Streams audio to the server via SocketIO.
    -   Captures images on button press and uploads to the server via HTTP.
    -   Plays back received TTS audio streams through the earbuds.

### 4.3. Mobile Application (Flutter)
A cross-platform app serving as the administrative interface.
-   **Dashboard**:
    -   Fetches speaker data (names, audio URLs, face images) from the backend.
    -   Categorizes entries into "Known" (Enrolled) and "Unknown".
-   **Speaker Management**:
    -   **Unknown Speakers**: Displays a list of auto-enrolled "Unknown" speakers (e.g., `Spk_20231201...`). Users can play the associated audio clip (fetched via `audio_url`) to recognize the voice and then update the name.
    -   **Face Management**: Displays images of recognized or auto-enrolled faces. Allows users to tag unknown faces with names.
-   **Integration**: Connects to the same backend databases (MongoDB/Pinecone) or API endpoints to update speaker/face metadata.

### 4.4. Audio Processing Pipeline (`stt_diarization/`)
Handles the conversion of raw audio into structured data.
-   **Transcription**: Uses `transcriber.py` (Whisper) for speech-to-text.
-   **Diarization**: Uses `diarizer.py` to segment audio by speaker.
-   **Speaker Identification**:
    -   Generates embeddings using SpeechBrain.
    -   **Auto-Enrollment**: If a speaker is unknown, they are assigned a temporary ID. Their audio sample is uploaded to Cloudinary, and the URL is stored in the database. This `audio_url` is what the Mobile App retrieves for playback.

### 4.5. Memory System (`context/`)
Manages storage and retrieval.
-   **Chunking**: Splits text into semantic chunks with metadata (Timestamp, Conversation ID).
-   **Vector Storage**:
    -   **Pinecone**: Stores embeddings for semantic search.
    -   **MongoDB**: Stores raw chunks and alerts for reliability and easy retrieval by the Mobile App.
-   **Retrieval**: Uses Gemini 2.0 Flash to generate answers based on retrieved context.

### 4.6. Face Recognition System
-   **Process**: Detects faces -> Generates Embeddings -> Matches in Pinecone.
-   **Auto-Enrollment**: Unknown faces are saved as `Person_Timestamp`. The image is uploaded to Cloudinary, and the URL is stored.
-   **Mobile Integration**: The App displays these Cloudinary images, allowing the user to put a face to the name and update the label.

### 4.7. Automated Alert System (`context/alert_processing.py`)
A proactive feature that extracts actionable items from conversations.
-   **Extraction**: Uses **Gemini** to analyze conversation text and identify requests for reminders, alarms, or notifications.
-   **Structured Data**: Extracts specific fields: `alert_text`, `date`, `time`, `type` (reminder/alert/notification), and `confidence` score.
-   **Storage**: High-confidence alerts (>0.6) are saved to **MongoDB** (`save_alert_to_mongodb`).
-   **Usage**: These alerts can be fetched by the Mobile App or Client to set actual system alarms or push notifications.

## 5. Technology Stack
-   **Server**: Python, Flask, Flask-SocketIO
-   **Client**: Raspberry Pi (Linux), Python Client Scripts
-   **Mobile App**: Flutter (Dart)
-   **AI Models**: OpenAI Whisper (STT), Gemini 2.0 Flash (LLM), SpeechBrain (Voice ID), dlib (Face ID), Edge-TTS
-   **Databases**: Pinecone (Vector), MongoDB (NoSQL)
-   **Storage**: Cloudinary (Images/Audio)
