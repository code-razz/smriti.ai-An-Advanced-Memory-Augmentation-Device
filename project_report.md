# Project Report: AISmriti - Advanced Memory Augmentation Device

## 1. Executive Summary
**AISmriti** is an advanced voice assistant and memory augmentation system designed to capture, process, and recall conversations. It functions as a "second brain" by continuously listening to conversations (Context Mode), identifying speakers, and storing structured memories in a vector database. Users can interact with the system via voice queries (Query Mode) to ask questions about past conversations, receiving intelligent, context-aware responses generated by a Large Language Model (LLM).

The system is built with a **Flask-SocketIO** backend for real-time audio streaming, utilizes **Whisper** for transcription, **Pyannote** for speaker diarization, **Pinecone** & **MongoDB** for memory storage, and **Google Gemini** for response generation.

---

## 2. System Architecture

### 2.1 High-Level Components
*   **Client**: (Presumably a mobile app or web interface) Streams audio to the server and plays back responses.
*   **Server (`server.py`)**: The central hub handling WebSocket connections, audio buffering, and orchestration of processing pipelines.
*   **Audio Processing Engine (`stt_diarization/`)**: Handles Speech-to-Text (STT), Speaker Diarization, and Speaker Identification.
*   **Context Manager (`context/`)**: Manages text chunking, embedding generation, and database interactions.
*   **Memory Store**:
    *   **Pinecone**: Vector database for semantic search of conversation history.
    *   **MongoDB**: Document store for raw conversation chunks and metadata.
*   **LLM & TTS**:
    *   **Google Gemini**: Generates natural language responses based on retrieved context.
    *   **Edge-TTS**: Converts text responses to speech for audio playback.

### 2.2 Tech Stack
*   **Language**: Python 3.x
*   **Web Framework**: Flask, Flask-SocketIO
*   **AI/ML Models**:
    *   **Transcription**: OpenAI Whisper
    *   **Diarization**: Pyannote Audio
    *   **Speaker Embeddings**: SpeechBrain (ECAPA-TDNN)
    *   **Text Embeddings**: Cohere (`embed-english-v3.0`)
    *   **LLM**: Google Gemini 2.0 Flash
*   **Databases**: Pinecone (Vector), MongoDB (NoSQL)
*   **Cloud Services**: Cloudinary (Audio storage for speaker profiles)

---

## 3. Key Workflows

### 3.1 Real-Time Query Mode
1.  **Input**: User presses a button and speaks a question.
2.  **Streaming**: Audio is streamed to `server.py` via `audio_chunk` events.
3.  **Processing**:
    *   Audio is saved to a WAV file.
    *   **STT**: Transcribed using Whisper (In-Memory).
    *   **Retrieval**: System searches Pinecone for relevant conversation chunks based on the query.
    *   **Generation**: Gemini generates an answer using the retrieved context.
4.  **Output**: The answer is streamed back to the client as audio (TTS) and text.

### 3.2 Context Recording Mode (Passive Listening)
1.  **Input**: User activates "Context Mode".
2.  **Streaming**: Audio is continuously streamed to `server.py` via `context_audio_chunk`.
3.  **Segmentation**: Audio is buffered and processed in 50-second segments (with overlap).
4.  **Processing**:
    *   **STT & Diarization**: Converts audio to a transcript with speaker labels (e.g., "Speaker A: Hello").
    *   **Speaker ID**: Identifies speakers using voice fingerprints (embeddings).
5.  **Storage**:
    *   Transcript is split into semantic chunks.
    *   Chunks are embedded (Cohere) and stored in Pinecone & MongoDB.

---

## 4. Comprehensive Implementation Details

### 4.1 Server & Real-Time Handling (`server.py`)
*   **SocketIO Events**:
    *   `audio_chunk` / `audio_complete`: Handles user queries.
    *   `context_audio_chunk`: Handles background conversation recording.
*   **Concurrency**: Uses `threading` and `socketio.start_background_task` to process audio without blocking the main event loop.
*   **Buffering**: Maintains per-client buffers (`partial_audio`, `partial_context_audio`) to accumulate audio before processing.
*   **Streaming Response**: Implements a sophisticated pipeline where the LLM generates text chunks, which are immediately sent to TTS, and the resulting audio is streamed to the client, minimizing latency.

### 4.2 Audio Processing Pipeline (`stt_diarization/`)
*   **Transcription**: Uses `transcriber.py` (Whisper) to get word-level timestamps.
*   **Diarization**: Uses `diarizer.py` (Pyannote) to segment audio by speaker turns.
*   **Alignment**: `process_audio.py` aligns Whisper word timestamps with Pyannote speaker segments to create a "Speaker: Text" transcript.
*   **Streaming Support**: `process_audio_streaming.py` handles short audio segments, maintaining context and speaker consistency across chunks.

### 4.3 Speaker Identification System (`core_processing.py`)
*   **Embeddings**: Generates 192-dim vectors using `speechbrain/spkrec-ecapa-voxceleb`.
*   **Identification Logic**:
    1.  **Local Cache**: Checks a local dictionary of recently seen speakers for a high-similarity match (Cosine Similarity > Threshold).
    2.  **Vector DB**: If no local match, queries Pinecone for known speakers.
    3.  **Enrollment**: If still unknown, enrolls the speaker as `Spk_{Timestamp}_{UUID}`, saves their voice sample to Cloudinary, and indexes their embedding.

### 4.4 Context Management & Storage (`context/process_chunks.py`)
*   **Chunking Strategy**:
    *   Splits conversation into utterances.
    *   Groups sentences into chunks of ~900 characters.
    *   Preserves speaker labels to maintain context.
*   **Dual-Write Architecture**:
    *   **Pinecone**: Stores the chunk text + vector embedding for semantic search.
    *   **MongoDB**: Stores the exact same chunk structure for reliability and potential non-vector queries.
*   **Metadata**: Each chunk includes `conversation_id`, `timestamp`, `participants`, and `chunk_index`.

### 4.5 Intelligent Response Generation (`context/response.py`)
*   **Retrieval Augmented Generation (RAG)**:
    *   Takes the user's query.
    *   Searches Pinecone for the most relevant conversation chunks.
    *   Passes these chunks as "Memories" to the LLM.
*   **Prompt Engineering**: Uses a structured prompt (`RESPONDER_PROMPT`) that instructs Gemini to answer based *only* on the provided memories, citing speakers and context where possible.
*   **Streaming**: Uses `generate_content(stream=True)` to yield text tokens instantly for the TTS pipeline.

### 4.6 Database Integration (`db_utils.py`, `mongodb_client.py`)
*   **MongoDB**: Uses `pymongo` to connect to a cloud MongoDB instance.
*   **Synchronization**: The `save_chunk_to_mongodb` function ensures that every chunk written to Pinecone is also backed up to MongoDB using the same ID, ensuring data consistency.

---

## 5. Conclusion
The project is a sophisticated implementation of a modern AI assistant. It successfully integrates multiple complex AI disciplines—speech processing, speaker recognition, vector search, and generative AI—into a cohesive real-time system. The architecture is modular, allowing for independent scaling or upgrading of components (e.g., swapping the LLM or STT model).
